{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM in `pyensmallen+jax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import pyensmallen as pe\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear IV\n",
    "\n",
    "The linear moment condition $z (y - x\\beta)$ is attached as a static-method (`iv_moment`) to the class for convenience. This covers OLS and 2SLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for IV estimation\n",
    "def generate_test_data(n=5000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Generate instruments\n",
    "    z1 = np.random.normal(0, 1, n)\n",
    "    z2 = np.random.normal(0, 1, n)\n",
    "    Z = np.column_stack([np.ones(n), z1, z2])\n",
    "\n",
    "    # Generate error terms with correlation\n",
    "    error = np.random.normal(0, 1, n)\n",
    "    v = 0.7 * error + np.random.normal(0, 0.5, n)\n",
    "\n",
    "    # Generate endogenous variable\n",
    "    x = 0.5 * z1 - 0.2 * z2 + v\n",
    "    X = np.column_stack([np.ones(n), x])\n",
    "\n",
    "    # Generate outcome\n",
    "    true_beta = np.array([-0.5, 1.2])\n",
    "    y = X @ true_beta + error\n",
    "\n",
    "    return y, X, Z, true_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GMM Results:\n",
      "True parameters: [-0.5  1.2]\n",
      "Estimated parameters: [-0.48933885  1.19956026]\n",
      "Standard errors: [0.01412415 0.02603365]\n"
     ]
    }
   ],
   "source": [
    "# Generate test data\n",
    "y, X, Z, true_beta = generate_test_data()\n",
    "\n",
    "# Create and fit GMM estimator\n",
    "gmm = pe.EnsmallenEstimator(pe.EnsmallenEstimator.iv_moment, \"optimal\")\n",
    "gmm.fit(Z, y, X, verbose=True)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nGMM Results:\")\n",
    "print(f\"True parameters: {true_beta}\")\n",
    "print(f\"Estimated parameters: {gmm.theta_}\")\n",
    "print(f\"Standard errors: {gmm.std_errors_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Fast Score Bootstrap\n",
    "\n",
    "The \"slow\" or \"full\" non-parametric bootstrap is conceptually simple:\n",
    "1.  Pretend the sample is the true population.\n",
    "2.  Draw a new sample (with replacement) from this \"population\".\n",
    "3.  Re-calculate your estimator $\\hat{\\theta}$ from scratch on this new sample.\n",
    "4.  Repeat 1000s of times to build a distribution of $\\hat{\\theta}^*$. The standard deviation of this distribution is your standard error.\n",
    "\n",
    "This is robust but computationally expensive because of Step 3. The \"fast\" score bootstrap is a clever shortcut that avoids re-running the full optimization. It's based on a first-order Taylor approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Logic:**\n",
    "\n",
    "1.  **The Starting Point:** The M-estimator $\\hat{\\theta}$ is defined by the first-order condition (FOC) that the sample average of the moment conditions is (approximately) zero:\n",
    "    \n",
    "$$\n",
    "(1/n) \\sum g(z_i, \\hat{\\theta}) \\approx 0\n",
    "$$\n",
    "\n",
    "2.  **The Bootstrap Sample:** Now, imagine we have a bootstrap sample. The bootstrap estimate $\\hat{\\theta}^*$ would be the one that solves the FOC for *that* sample:\n",
    "    \n",
    "$$\n",
    "(1/n) \\sum g(z_i^*, \\hat{\\theta}^*) \\approx 0 \n",
    "$$\n",
    "    \n",
    "where $z_i^*$ are draws from the original sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  **The Taylor Expansion Trick:** We don't want to actually *solve* for $\\hat{\\theta}^*$. Instead, we can approximate it. Let's do a first-order Taylor expansion of the bootstrap FOC around the *original* estimate $\\hat{\\theta}$:\n",
    "    \n",
    "$$\n",
    "(1/n) \\sum g(z_i^*, \\hat{\\theta}^*) \\approx (1/n) \\sum g(z_i^*, \\hat{\\theta}) + [ (1/n) \\sum (\\partial g(z_i^*, \\theta)/\\partial \\theta) \\mid_{\\hat{\\theta}} ]  (\\hat{\\theta}^* - \\hat{\\theta})\n",
    "$$\n",
    "    \n",
    "Since the left side is zero, we can rearrange to solve for the difference $(\\hat{\\theta}^* - \\hat{\\theta})$ \n",
    "\n",
    "$$\n",
    "(\\hat{\\theta}^* - \\hat{\\theta}) \\approx\n",
    "- [ (1/n) \\sum (\\partial g(z_i^*, \\theta)/\\partial \\theta) \\mid_{\\hat{\\theta}} ]^{-1} \n",
    "\\; \\;  [ (1/n) \\sum g(z_i^*, \\hat{\\theta}) ]\n",
    "$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  **Simplifying the Approximation:**\n",
    "*   The first term $[ (1/n) \\sum (\\partial g(z_i^*, \\theta)/\\partial \\theta) \\mid_{\\hat{\\theta}}]$  is the average Jacobian of the moments, evaluated on the bootstrap sample. By the Law of Large Numbers, this is approximately the same as the Jacobian on the original sample, which we call $.\n",
    "*   The second term $(1/n) \\sum g(z_i^*, \\hat{\\theta})$  is simply the average of the *original* moment residuals $g_i(\\hat{\\theta})$ over the bootstrap sample. Let's call this $\\bar{g}^*$.\n",
    "\n",
    "This simplifies our approximation to:\n",
    "\n",
    "\n",
    "$$\n",
    "(\\hat{\\theta}^* - \\hat{\\theta}) \\approx - G^{-1} \\cdot \\bar{g}^*\n",
    "$$\n",
    "\n",
    "(This is for the just-identified case. For GMM, the full term is $-(G'WG)^{-1} G'W \\; \\cdot \\; \\bar{g}^*$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Fast Bootstrap Algorithm:**\n",
    "\n",
    "This approximation is the heart of the fast bootstrap:\n",
    "\n",
    "1.  Calculate the original estimate $\\hat{\\theta}$ once.\n",
    "2.  Calculate the $n$ individual moment residuals $g_i(\\hat{\\theta})$ once.\n",
    "3.  Calculate the matrix $M = -(G'WG)^{-1}G'W$ once.\n",
    "4.  Loop $B$ times:\n",
    "        (a) Create a bootstrap sample of the *residuals* $g_i$ (not the data).\n",
    "        (b) Calculate their mean, $\\bar{g}^*$.\n",
    "        (c) Calculate the bootstrap estimate directly: $\\hat{\\theta}^* = \\hat{\\theta} + M * \\bar{g}^*$.\n",
    "5.  Compute the standard deviation of the $B$ values of $\\hat{\\theta}^*$.\n",
    "\n",
    "This is orders of magnitude faster because the loop only involves simple matrix-vector products, not a full re-optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to Newey (1994) Efficiency Framework\n",
    "\n",
    "The connection is the **influence function**, and is central to Newey (1994).\n",
    "\n",
    "*   **What is the Influence Function?** The influence function $\\varphi(z_i)$  for an estimator $\\hat{\\theta}$ tells you how much the estimate changes when you add a single observation $z_i$ to your sample. It's the building block of the estimator's asymptotic distribution:\n",
    "    \n",
    "$$\n",
    "\\sqrt{n} (\\hat{\\theta} - \\theta_0) = \\frac{1}{\\sqrt{n}} \\sum \\varphi(z_i) + o_p(1)\n",
    "$$\n",
    "\n",
    "*   **The Influence Function for GMM:** the influence function for a GMM estimator is:\n",
    "\n",
    "$$\n",
    "\\varphi(z_i) = - (G'WG)^{-1} G'W \\cdot g(z_i, \\theta_0)\n",
    "$$\n",
    "\n",
    "\n",
    "The core of the fast bootstrap approximation is a sample-based version of the influence function. The matrix $M = -(G'WG)^{-1}G'W$ is the key component that transforms the moment conditions $g$ into their effect on the parameter estimate $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it relates to Newey's Efficiency Analysis :**\n",
    "\n",
    "Newey's framework is all about comparing the influence functions of different estimators.\n",
    "\n",
    "1.  **A Common Structure:** Newey  classifies estimators by writing their influence functions in a general form:\n",
    "    \n",
    "$$\n",
    "\\varphi(z, \\tau) = D(\\tau)^{-1} m(z, \\tau)\n",
    "$$\n",
    "    \n",
    "where $\\tau$ is some parameter indexing the class of estimators (e.g., for GMM, $\\tau$ could be the weighting matrix $W$). $D$ is a non-stochastic matrix (like $G'WG$) and $m$ is a zero-mean function (like $G'Wg$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  **The Efficiency Condition:** An estimator indexed by $\\bar{\\tau}$ is efficient within its class if its influence function $\\varphi(z, \\bar{\\tau})$ is \"as small as possible\". The key condition is that for an efficient estimator, its $D$ matrix must equal the variance of its $m$ function:\n",
    "\n",
    "$$\n",
    "D(\\bar{\\tau}) = E[m(z, \\bar{\\tau})  m(z, \\bar{\\tau})']\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  **Optimal GMM:** For GMM, $D(W) = G'WG$ and $m(z, W) = G'Wg(z)$. The variance of $m$ is $E[(G'Wg)(G'Wg)'] = G'W \\Omega WG$. The efficiency condition becomes:\n",
    "    \n",
    "$$\n",
    "G'WG = G'W \\Omega WG\n",
    "$$\n",
    "    \n",
    "This holds if $W = \\Omega^{-1}$. This proves that the GMM estimator using the inverse variance of the moments as the weighting matrix is efficient *within the class of GMM estimators defined by those moments*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 177 ms, sys: 1.91 ms, total: 179 ms\n",
      "Wall time: 178 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get fast bootstrap standard errors\n",
    "fast_bootstrap_se = gmm.bootstrap_scores(n_bootstrap=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.5 s, sys: 968 ms, total: 49.5 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "slow_bootstrap_se = gmm.bootstrap_full(n_bootstrap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True</th>\n",
       "      <th>GMM</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Fast Bootstrap</th>\n",
       "      <th>Full Bootstrap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.489339</td>\n",
       "      <td>0.014124</td>\n",
       "      <td>0.014096</td>\n",
       "      <td>0.013001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2</td>\n",
       "      <td>1.199560</td>\n",
       "      <td>0.026034</td>\n",
       "      <td>0.025503</td>\n",
       "      <td>0.024544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   True       GMM  Analytic  Fast Bootstrap  Full Bootstrap\n",
       "0  -0.5 -0.489339  0.014124        0.014096        0.013001\n",
       "1   1.2  1.199560  0.026034        0.025503        0.024544"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(\n",
    "    np.c_[true_beta, gmm.theta_, gmm.std_errors_, fast_bootstrap_se, slow_bootstrap_se],\n",
    "    columns=[\"True\", \"GMM\", \"Analytic\", \"Fast Bootstrap\", \"Full Bootstrap\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear GMM: Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit DGP\n",
    "n = 1000\n",
    "p = 2\n",
    "X = np.random.normal(size=(n, p))\n",
    "X = np.c_[np.ones(n), X]\n",
    "beta = np.array([0.5, -0.5, 0.5])\n",
    "y = np.random.binomial(1, 1 / (1 + np.exp(-X @ beta)))\n",
    "Z = X.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IWLS solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  [ 0.45130964 -0.44964216  0.53112315]\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "logit_mod = sm.Logit(y, X)\n",
    "logit_res = logit_mod.fit(disp=0)\n",
    "print(\"Parameters: \", logit_res.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nonlinear GMM with ensmallen\n",
    "\n",
    "define moment condition (in jax-compatible terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.scipy.special as jsp\n",
    "\n",
    "def ψ_logit(z, y, x, beta):\n",
    "    # Use jax.scipy.special.expit instead of scipy.special.expit\n",
    "    resid = y - jsp.expit(x @ beta)\n",
    "    return z * resid[:, jnp.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameters: [ 0.5 -0.5  0.5]\n",
      "Estimated parameters: [ 0.45130965 -0.44964226  0.53112299]\n",
      "Standard errors: [0.06921393 0.07012597 0.06711118]\n"
     ]
    }
   ],
   "source": [
    "# Create and fit GMM estimator\n",
    "gmm = pe.EnsmallenEstimator(ψ_logit, \"optimal\")\n",
    "gmm.fit(Z, y, X, verbose=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"True parameters: {beta}\")\n",
    "print(f\"Estimated parameters: {gmm.theta_}\")\n",
    "print(f\"Standard errors: {gmm.std_errors_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 857 ms, sys: 21.4 ms, total: 878 ms\n",
      "Wall time: 696 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get fast bootstrap standard errors\n",
    "bootstrap_se = gmm.bootstrap_scores(n_bootstrap=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.45130965,  0.06921393,  0.07022184],\n",
       "       [-0.44964226,  0.07012597,  0.07125831],\n",
       "       [ 0.53112299,  0.06711118,  0.06535547]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.c_[gmm.theta_, gmm.std_errors_, bootstrap_se]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
