{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pyensmallen\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(123)\n",
    "key = jax.random.PRNGKey(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "def generate_data(n_samples=1000, beta_true=None, corr_factor=0.5):\n",
    "    \"\"\"Generate data for IV regression with endogenous regressors\"\"\"\n",
    "    # Number of instruments and endogenous variables\n",
    "    n_instruments = 10\n",
    "    n_endogenous = 2\n",
    "\n",
    "    if beta_true is None:\n",
    "        beta_true = np.array([1.0, -0.5])\n",
    "\n",
    "    # Generate instruments Z ~ N(0, 1)\n",
    "    Z = np.random.normal(0, 1, (n_samples, n_instruments))\n",
    "\n",
    "    # Generate endogenous X related to Z\n",
    "    X = np.zeros((n_samples, n_endogenous))\n",
    "    # First stage: X = Z*Pi + V where V is correlated with the error in Y\n",
    "    Pi = np.random.uniform(-1, 1, (n_instruments, n_endogenous))\n",
    "\n",
    "    # Error terms - generate with correlation structure\n",
    "    error_cov = np.eye(n_endogenous + 1)\n",
    "    # Make first column/row correlate with others to create endogeneity\n",
    "    error_cov[0, 1:] = corr_factor\n",
    "    error_cov[1:, 0] = corr_factor\n",
    "\n",
    "    errors = np.random.multivariate_normal(\n",
    "        np.zeros(n_endogenous + 1), error_cov, n_samples\n",
    "    )\n",
    "\n",
    "    # Structural errors\n",
    "    u = errors[:, 0]\n",
    "    # First stage errors\n",
    "    V = errors[:, 1:]\n",
    "\n",
    "    # First stage\n",
    "    X = Z @ Pi + V\n",
    "\n",
    "    # Outcome equation\n",
    "    y = X @ beta_true + u\n",
    "\n",
    "    return y, X, Z, beta_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "n_samples = 1000\n",
    "true_beta = np.array([1.0, -0.5])\n",
    "y, X, Z, beta_true = generate_data(n_samples, true_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GMM estimation with JAX \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define moment conditions: E[Z'(y - X*beta)] = 0\n",
    "@jax.jit\n",
    "def moment_conditions(beta, y, X, Z):\n",
    "    \"\"\"Moment conditions for IV regression: Z'(y - X*beta) = 0\"\"\"\n",
    "    residuals = y - X @ beta\n",
    "    return (Z.T @ residuals) / Z.shape[0]\n",
    "\n",
    "# Define GMM objective function\n",
    "@jax.jit\n",
    "def gmm_objective(beta, W, y, X, Z):\n",
    "    \"\"\"GMM objective: g'*W*g where g = Z'(y-X*beta)\"\"\"\n",
    "    g = moment_conditions(beta, y, X, Z)\n",
    "    return g.T @ W @ g\n",
    "\n",
    "# Create JAX gradient function for the GMM objective\n",
    "grad_gmm = jax.grad(gmm_objective)\n",
    "\n",
    "# Vectorized matrix sandwich formula for asymptotic variance\n",
    "@jax.jit\n",
    "def asymptotic_variance(beta, y, X, Z, W):\n",
    "    \"\"\"Compute asymptotic variance for GMM estimator\"\"\"\n",
    "    n = Z.shape[0]\n",
    "    residuals = y - X @ beta\n",
    "\n",
    "    # Create individual moment conditions (Z_i * e_i)\n",
    "    g_i = Z.T * residuals.reshape(1, -1)  # Shape: (n_instruments, n_samples)\n",
    "\n",
    "    # Compute gradient of moment conditions (Jacobian): E[Z'X]\n",
    "    G = -Z.T @ X / n  # Shape: (n_instruments, n_endogenous)\n",
    "\n",
    "    # Compute S matrix (covariance of moment conditions)\n",
    "    S = (g_i @ g_i.T) / n  # Shape: (n_instruments, n_instruments)\n",
    "\n",
    "    # Compute bread for sandwich formula: (G'WG)^{-1}\n",
    "    bread = jnp.linalg.inv(G.T @ W @ G)\n",
    "\n",
    "    # Compute meat for sandwich formula: G'WSW'G\n",
    "    meat = G.T @ W @ S @ W @ G\n",
    "\n",
    "    # Asymptotic variance: (G'WG)^{-1} * (G'WSW'G) * (G'WG)^{-1} / n\n",
    "    avar = bread @ meat @ bread / n\n",
    "\n",
    "    return avar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function for pyensmallen\n",
    "def objective_for_ensmallen(beta, gradient, y, X, Z, W):\n",
    "    \"\"\"Objective for pyensmallen with JAX gradient calculation\"\"\"\n",
    "    beta_jax = jnp.array(beta)\n",
    "\n",
    "    # Calculate objective value\n",
    "    obj_value = gmm_objective(beta_jax, W, y, X, Z)\n",
    "\n",
    "    # Calculate gradient\n",
    "    grad = grad_gmm(beta_jax, W, y, X, Z)\n",
    "\n",
    "    # Copy gradient to output parameter\n",
    "    gradient[:] = np.array(grad)\n",
    "\n",
    "    return float(obj_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_step_gmm(\n",
    "    y, X, Z, initial_beta=None, use_joblib_bootstrap=True, n_bootstrap=200\n",
    "):\n",
    "    \"\"\"Two-step GMM estimation for IV regression\"\"\"\n",
    "    n_instruments = Z.shape[1]\n",
    "    n_endogenous = X.shape[1]\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Convert data to JAX arrays for later use\n",
    "    y_jax = jnp.array(y)\n",
    "    X_jax = jnp.array(X)\n",
    "    Z_jax = jnp.array(Z)\n",
    "\n",
    "    # Initial weight matrix: identity\n",
    "    W_1 = jnp.eye(n_instruments)\n",
    "\n",
    "    # Initial guess if not provided\n",
    "    if initial_beta is None:\n",
    "        initial_beta = np.zeros(n_endogenous)\n",
    "\n",
    "    # First step GMM\n",
    "    start_time = time.time()\n",
    "    optimizer = pyensmallen.L_BFGS(10, 1000)  # numBasis, maxIterations\n",
    "\n",
    "    # Define objective function for first step\n",
    "    def first_step_objective(beta, gradient):\n",
    "        return objective_for_ensmallen(beta, gradient, y_jax, X_jax, Z_jax, W_1)\n",
    "\n",
    "    # Optimize first step\n",
    "    beta_1 = optimizer.optimize(first_step_objective, initial_beta)\n",
    "\n",
    "    # Compute optimal weight matrix using first-step residuals\n",
    "    residuals = y - X @ beta_1\n",
    "    g_i = Z.T * residuals.reshape(1, -1)  # Individual moment contributions\n",
    "    S = (g_i @ g_i.T) / n_samples  # Covariance of moment conditions\n",
    "\n",
    "    # Optimal weight matrix: W = S^{-1}\n",
    "    W_opt = jnp.linalg.inv(S)\n",
    "\n",
    "    # Second step GMM with optimal weight matrix\n",
    "    def second_step_objective(beta, gradient):\n",
    "        return objective_for_ensmallen(beta, gradient, y_jax, X_jax, Z_jax, W_opt)\n",
    "\n",
    "    # Optimize second step\n",
    "    beta_2 = optimizer.optimize(second_step_objective, beta_1)\n",
    "    estimation_time = time.time() - start_time\n",
    "\n",
    "    # Compute analytical standard errors\n",
    "    avar = asymptotic_variance(beta_2, y_jax, X_jax, Z_jax, W_opt)\n",
    "    se_analytical = np.sqrt(np.diag(avar))\n",
    "\n",
    "    # Bootstrap standard errors\n",
    "    if use_joblib_bootstrap:\n",
    "        # Bootstrap function\n",
    "        def bootstrap_sample(b):\n",
    "            # Sample with replacement\n",
    "            idx = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            y_b, X_b, Z_b = y[idx], X[idx], Z[idx]\n",
    "\n",
    "            # Convert to JAX arrays\n",
    "            y_b_jax = jnp.array(y_b)\n",
    "            X_b_jax = jnp.array(X_b)\n",
    "            Z_b_jax = jnp.array(Z_b)\n",
    "\n",
    "            # First step with identity weight matrix\n",
    "            def bs_first_objective(beta, gradient):\n",
    "                return objective_for_ensmallen(\n",
    "                    beta, gradient, y_b_jax, X_b_jax, Z_b_jax, W_1\n",
    "                )\n",
    "\n",
    "            beta_b1 = optimizer.optimize(bs_first_objective, initial_beta)\n",
    "\n",
    "            # Compute optimal weight matrix for this bootstrap sample\n",
    "            residuals_b = y_b - X_b @ beta_b1\n",
    "            g_i_b = Z_b.T * residuals_b.reshape(1, -1)\n",
    "            S_b = (g_i_b @ g_i_b.T) / n_samples\n",
    "            W_b = jnp.linalg.inv(S_b)\n",
    "\n",
    "            # Second step with optimal weight matrix\n",
    "            def bs_second_objective(beta, gradient):\n",
    "                return objective_for_ensmallen(\n",
    "                    beta, gradient, y_b_jax, X_b_jax, Z_b_jax, W_b\n",
    "                )\n",
    "\n",
    "            beta_b2 = optimizer.optimize(bs_second_objective, beta_b1)\n",
    "            return beta_b2\n",
    "\n",
    "        # Run bootstrap in parallel\n",
    "        start_bootstrap = time.time()\n",
    "        bootstrap_results = Parallel(n_jobs=-1)(\n",
    "            delayed(bootstrap_sample)(b) for b in range(n_bootstrap)\n",
    "        )\n",
    "        bootstrap_time = time.time() - start_bootstrap\n",
    "\n",
    "        # Compute bootstrap standard errors\n",
    "        bootstrap_estimates = np.array(bootstrap_results)\n",
    "        se_bootstrap = np.std(bootstrap_estimates, axis=0)\n",
    "\n",
    "        # Compute bootstrap confidence intervals\n",
    "        ci_bootstrap = np.percentile(bootstrap_estimates, [2.5, 97.5], axis=0).T\n",
    "    else:\n",
    "        se_bootstrap = None\n",
    "        ci_bootstrap = None\n",
    "        bootstrap_time = 0\n",
    "\n",
    "    # Compute t-stats and p-values using analytical standard errors\n",
    "    t_stats = beta_2 / se_analytical\n",
    "    p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), df=n_samples - n_endogenous))\n",
    "\n",
    "    # Analytical confidence intervals\n",
    "    ci_analytical = np.column_stack(\n",
    "        [beta_2 - 1.96 * se_analytical, beta_2 + 1.96 * se_analytical]\n",
    "    )\n",
    "\n",
    "    # Combine results\n",
    "    results = {\n",
    "        \"beta_first_step\": beta_1,\n",
    "        \"beta\": beta_2,\n",
    "        \"se_analytical\": se_analytical,\n",
    "        \"se_bootstrap\": se_bootstrap,\n",
    "        \"ci_analytical\": ci_analytical,\n",
    "        \"ci_bootstrap\": ci_bootstrap,\n",
    "        \"p_values\": p_values,\n",
    "        \"t_stats\": t_stats,\n",
    "        \"estimation_time\": estimation_time,\n",
    "        \"bootstrap_time\": bootstrap_time,\n",
    "        \"n_bootstrap\": n_bootstrap if use_joblib_bootstrap else 0,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GMM IV Estimation Results ---\n",
      "Sample size: 1000\n",
      "Number of instruments: 10\n",
      "Number of endogenous variables: 2\n",
      "\n",
      "True coefficients: [ 1.  -0.5]\n",
      "First-step estimates: [ 0.98635808 -0.48494819]\n",
      "Second-step estimates: [ 0.98251398 -0.48098669]\n",
      "\n",
      "Analytical standard errors:\n",
      "  Beta_1: 0.015188\n",
      "  Beta_2: 0.022259\n",
      "{'beta_first_step': array([ 0.98635808, -0.48494819]), 'beta': array([ 0.98251398, -0.48098669]), 'se_analytical': array([0.01518814, 0.02225898], dtype=float32), 'se_bootstrap': None, 'ci_analytical': array([[ 0.95274521,  1.01228274],\n",
      "       [-0.52461429, -0.43735909]]), 'ci_bootstrap': None, 'p_values': array([0., 0.]), 't_stats': array([ 64.68953306, -21.6086579 ]), 'estimation_time': 0.10393977165222168, 'bootstrap_time': 0, 'n_bootstrap': 0}\n"
     ]
    }
   ],
   "source": [
    "# Estimate with two-step GMM\n",
    "results = two_step_gmm(y, X, Z, use_joblib_bootstrap=False, n_bootstrap=500)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n--- GMM IV Estimation Results ---\")\n",
    "print(f\"Sample size: {n_samples}\")\n",
    "print(f\"Number of instruments: {Z.shape[1]}\")\n",
    "print(f\"Number of endogenous variables: {X.shape[1]}\")\n",
    "print(f\"\\nTrue coefficients: {true_beta}\")\n",
    "print(f\"First-step estimates: {results['beta_first_step']}\")\n",
    "print(f\"Second-step estimates: {results['beta']}\")\n",
    "print(\"\\nAnalytical standard errors:\")\n",
    "for i, se in enumerate(results[\"se_analytical\"]):\n",
    "    print(f\"  Beta_{i+1}: {se:.6f}\")\n",
    "\n",
    "print(results)\n",
    "# print(\"\\nBootstrap standard errors:\")\n",
    "# for i, se in enumerate(results[\"se_bootstrap\"]):\n",
    "#     print(f\"  Beta_{i+1}: {se:.6f}\")\n",
    "\n",
    "# print(\"\\nAnalytical 95% CI:\")\n",
    "# for i, ci in enumerate(results[\"ci_analytical\"]):\n",
    "#     print(f\"  Beta_{i+1}: [{ci[0]:.6f}, {ci[1]:.6f}]\")\n",
    "\n",
    "# print(\"\\nBootstrap 95% CI:\")\n",
    "# for i, ci in enumerate(results[\"ci_bootstrap\"]):\n",
    "#     print(f\"  Beta_{i+1}: [{ci[0]:.6f}, {ci[1]:.6f}]\")\n",
    "\n",
    "# print(\"\\nP-values:\")\n",
    "# for i, p in enumerate(results[\"p_values\"]):\n",
    "#     print(f\"  Beta_{i+1}: {p:.6f}\")\n",
    "\n",
    "# print(f\"\\nEstimation time: {results['estimation_time']:.2f} seconds\")\n",
    "# print(\n",
    "#     f\"Bootstrap time ({results['n_bootstrap']} replications): {results['bootstrap_time']:.2f} seconds\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Visualization -------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram of bootstrap estimates\n",
    "bootstrap_estimates = np.array([results[\"beta\"] for _ in range(results[\"n_bootstrap\"])])\n",
    "if results[\"se_bootstrap\"] is not None:\n",
    "    fig, axes = plt.subplots(1, X.shape[1], figsize=(12, 4))\n",
    "    for i in range(X.shape[1]):\n",
    "        ax = axes[i]\n",
    "        ax.hist(bootstrap_estimates[:, i], bins=30, alpha=0.7)\n",
    "        ax.axvline(true_beta[i], color=\"red\", linestyle=\"--\", label=\"True\")\n",
    "        ax.axvline(results[\"beta\"][i], color=\"blue\", linestyle=\"-\", label=\"GMM\")\n",
    "        ax.axvline(\n",
    "            results[\"ci_bootstrap\"][i, 0], color=\"green\", linestyle=\":\", label=\"95% CI\"\n",
    "        )\n",
    "        ax.axvline(results[\"ci_bootstrap\"][i, 1], color=\"green\", linestyle=\":\")\n",
    "        ax.set_title(f\"Bootstrap Distribution: Beta_{i+1}\")\n",
    "        if i == 0:\n",
    "            ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_estimates = np.array([results['beta'] for _ in range(results['n_bootstrap'])])\n",
    "if results['se_bootstrap'] is not None:\n",
    "    fig, axes = plt.subplots(1, X.shape[1], figsize=(12, 4))\n",
    "    for i in range(X.shape[1]):\n",
    "        ax = axes[i]\n",
    "        ax.hist(bootstrap_estimates[:, i], bins=30, alpha=0.7)\n",
    "        ax.axvline(true_beta[i], color='red', linestyle='--', label='True')\n",
    "        ax.axvline(results['beta'][i], color='blue', linestyle='-', label='GMM')\n",
    "        ax.axvline(results['ci_bootstrap'][i, 0], color='green', linestyle=':', label='95% CI')\n",
    "        ax.axvline(results['ci_bootstrap'][i, 1], color='green', linestyle=':')\n",
    "        ax.set_title(f'Bootstrap Distribution: Beta_{i+1}')\n",
    "        if i == 0:\n",
    "            ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare analytical vs bootstrap standard errors\n",
    "if results['se_bootstrap'] is not None:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    x = np.arange(X.shape[1])\n",
    "    width = 0.35\n",
    "    ax.bar(x - width/2, results['se_analytical'], width, label='Analytical')\n",
    "    ax.bar(x + width/2, results['se_bootstrap'], width, label='Bootstrap')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'Beta_{i+1}' for i in range(X.shape[1])])\n",
    "    ax.set_ylabel('Standard Error')\n",
    "    ax.set_title('Comparison of Standard Error Estimates')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
